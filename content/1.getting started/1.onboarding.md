---
title: Onboarding
description: Get started with Msty in a few easy steps
---

If you haven't downloaded Msty already, get started by downloading it first.

::card
---
to: https://msty.app?ref=docs
target: \_blank
---

#title
Download Msty

#content
Supported platforms: Windows, Mac, Linux.
::

::alert{type="info" icon="tabler:info-circle"}
  Once you have Msty installed and open, you can use **any** of the following steps to complete the onboarding.
::

::image-card{src="/getting-started/msty-onboarding-page.webp" alt="Msty Onboarding Page"}
::

## Setup Local AI
Use this option if you would like to use offline models and want Msty to manage them for you.

By default, Tiny Dolphin model (637MB) is selected for download due to its small size. You can also select a different model to onboard with that befits your system specifications.

## Add remote model providers (optional)
Use this option if you would like to use models from online providers.

Msty currently supports the following online model providers:

<br />

::card-group
  ::card
  ---
  title: Open AI
  icon: tabler:brand-openai
  ---
  ::

  ::card
  ---
  title: Mistral
  icon: logos:mistral-ai-icon
  ---
  ::

  ::card
  ---
  title: Anthropic (Claude)
  icon: logos:anthropic-icon
  ---
  ::

  ::card
  ---
  title: Google Gemini
  icon: simple-icons:googlegemini
  ---
  ::

  ::card
  ---
  title: Perplexity
  icon: simple-icons:perplexity
  ---
  ::

  ::card
  ---
  title: Open Router
  icon: icon-park-outline:right-branch-two
  ---
  ::

  ::card
  ---
  title: Groq
  icon: arcticons:letter-lowercase-circle-g
  ---
  ::  
::

::alert{type="info" icon="tabler:info-circle"}
  You may need to obtain the appropriate API key from your model provider to use its models in Msty.
::

In addition to the providers mentioned above, Msty also supports models from any Open AI compatible endpoints.

## Continue with Ollama models (advanced)
Use this option if you have already downloaded models through Ollama and would like to use them with Msty. This will set Msty's model path location to where Ollama saves its models on your computer.

Note: If you are onboarded with Local AI already and want to switch to using models from Ollama, you can read [how to use existing Ollama models](/how-to/use-existing-ollama-models) in Msty.
